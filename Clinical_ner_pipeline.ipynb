{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b744167",
   "metadata": {},
   "source": [
    "## Phase 1: Data Engineering & Clinical Preprocessing\n",
    "In this phase, we address the unique challenges of French clinical NLP. Standard NLP pipelines often fail in the healthcare domain due to:\n",
    "\n",
    "---\n",
    "\n",
    "* **Nested Entities:** Medical terms often overlap (e.g., \"Infarctus\" inside \"Infarctus du myocarde\").\n",
    "\n",
    "* **Sub-token Alignment:** Transformers like DrBERT split words into fragments, requiring us to align our labels manually.\n",
    "\n",
    "* **Class Imbalance:** In medical text, \"Outside\" tokens (non-medical words) vastly outnumber specific disease or drug entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cd4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: 2552 rows\n",
      "Sample words (Correctly Split): ['Insuffisance', 'gonadotrope', 'associ√©e', '√†', 'l']...\n",
      "Sample tags: [1, 1, 0, 0, 0]...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('quaero_clean.csv')\n",
    "\n",
    "def robust_word_tokenize(text_str):\n",
    "    # This regex finds everything inside single or double quotes\n",
    "    # It solves the issue where French words are mashed together in the CSV\n",
    "    return re.findall(r\"['\\\"](.*?)['\\\"]\", text_str)\n",
    "\n",
    "def robust_tag_tokenize(tag_str):\n",
    "    # Extracts all numbers from the string [1 1 0 0]\n",
    "    return [int(t) for t in re.findall(r'\\d+', tag_str)]\n",
    "\n",
    "# Apply the robust cleaning\n",
    "df['words'] = df['words'].apply(robust_word_tokenize)\n",
    "df['ner_tags'] = df['ner_tags'].apply(robust_tag_tokenize)\n",
    "\n",
    "# Filter out any rows where lengths don't match (Safety check)\n",
    "df = df[df['words'].map(len) == df['ner_tags'].map(len)]\n",
    "\n",
    "print(f\"Dataset Loaded: {len(df)} rows\")\n",
    "print(f\"Sample words (Correctly Split): {df.iloc[0]['words'][:5]}...\")\n",
    "print(f\"Sample tags: {df.iloc[0]['ner_tags'][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961625e",
   "metadata": {},
   "source": [
    "## Step 1: Handling Nested Entities (Longest-Match Strategy)\n",
    "Medical nomenclature is hierarchical. For a professional pipeline, we want to extract the most specific clinical term. If \"Cancer\" (6 letters) and \"Cancer du poumon\" (16 letters) are both present, we implement logic to prioritize the longest span. This prevents \"double-counting\" and improves clinical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86dc82b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with consistent word/tag alignment: 2552 / 2552\n"
     ]
    }
   ],
   "source": [
    "def resolve_nested_spans(words, tags):\n",
    "    \"\"\"\n",
    "    In QUAERO, sometimes tags are provided as a flat list. \n",
    "    In more complex scenarios with overlapping raw offsets, we would use \n",
    "    a 'longest-match' logic. For our token-based list, we ensure \n",
    "    consistency across the sequence.\n",
    "    \"\"\"\n",
    "    # This project uses the 'Flat' version of QUAERO which has already been pre-processed to prioritize the longest medical span.\n",
    "    # we verify that word count matches tag count.\n",
    "    if len(words) != len(tags):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Validation check\n",
    "df['is_valid'] = df.apply(lambda row: resolve_nested_spans(row['words'], row['ner_tags']), axis=1)\n",
    "print(f\"Rows with consistent word/tag alignment: {df['is_valid'].sum()} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbb019",
   "metadata": {},
   "source": [
    "## Step 2: DrBERT Tokenizer Alignment\n",
    "We use DrBERT, a specialized model pre-trained on the French NACHOS corpus. Because DrBERT uses WordPiece tokenization, a single word like \"hypoplasie\" might be split into ['hypo', '##plas', '##ie'].\n",
    "\n",
    "We must align our labels so that:\n",
    "\n",
    "* The first part of the word (hypo) gets the original tag.\n",
    "\n",
    "* The remaining parts (##plas, ##ie) get a special value of -100.\n",
    "\n",
    "* Why -100? PyTorch's loss function ignores labels with the value -100 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a950ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['Insuffisance', 'gonadotrope', 'associ√©e', '√†', 'l', ' ', ' ', 'cong√©nitale', '√†', 'forme', 'cytom√©galique', '.']\n",
      "Aligned Labels: [-100, 1, 1, -100, 0, 0, 0, 1, 1, 1, 0, 0, 3, -100, -100, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "#initialize the French Tokenizer\n",
    "model_checkpoint = \"Dr-BERT/DrBERT-7GB\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Test alignment on the first row\n",
    "test_row = {\"words\": [df.iloc[0]['words']], \"ner_tags\": [df.iloc[0]['ner_tags']]}\n",
    "test_aligned = tokenize_and_align_labels(test_row)\n",
    "\n",
    "print(\"Original Words:\", df.iloc[0]['words'])\n",
    "print(\"Aligned Labels:\", test_aligned[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154e49e",
   "metadata": {},
   "source": [
    "## Step 3: Class Balancing (Weighted Cross-Entropy)\n",
    "In French clinical reports, 80-90% of words are \"common\" (Outside/O-tag). If we train a model without weights, it will learn that it can get 90% accuracy by just guessing \"O\" for everything.\n",
    "\n",
    "To fix this, we calculate inverse frequency weights. This tells the model: \"If you miss a rare Disease or Chemical tag, the penalty is 20x higher than missing a common word.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a50ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Class Weights:\n",
      "Tag 0: 0.12\n",
      "Tag 1: 1.33\n",
      "Tag 2: 2.25\n",
      "Tag 3: 3.78\n",
      "Tag 4: 3.65\n",
      "Tag 5: 1.87\n",
      "Tag 6: 7.33\n",
      "Tag 7: 34.90\n",
      "Tag 8: 30.08\n",
      "Tag 9: 27.09\n",
      "Tag 10: 23.33\n"
     ]
    }
   ],
   "source": [
    "# Flatten all tags to count occurrences\n",
    "all_tags = [tag for tags_list in df['ner_tags'] for tag in tags_list]\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "# Calculate Weights: Total_Samples / (Num_Classes * Class_Count)\n",
    "total_tokens = sum(tag_counts.values())\n",
    "num_classes = 11  # Tags 0 to 10\n",
    "weights = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    count = tag_counts.get(i, 1) # Avoid division by zero\n",
    "    weight = total_tokens / (num_classes * count)\n",
    "    weights.append(weight)\n",
    "\n",
    "# Convert to Tensor for PyTorch\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "print(\"Calculated Class Weights:\")\n",
    "for i, w in enumerate(weights):\n",
    "    print(f\"Tag {i}: {w:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec1926",
   "metadata": {},
   "source": [
    "# Phase 2: Domain-Specific Fine-Tuning (Sovereign AI)\n",
    "In this phase, we move beyond generic models and standard evaluation. We will:\n",
    "\n",
    "1. **Model Selection:** Utilize DrBERT, a sovereign French model pre-trained on 7GB of medical text.\n",
    "\n",
    "2. **K-Fold Cross-Validation (5 Folds):** Instead of a single split, we will train 5 different models on 5 different subsets of data. This is the \"Gold Standard\" for proving robustness in clinical research.\n",
    "\n",
    "3. **Strict Evaluation:** We utilize the seqeval library to calculate Strict Entity-Level F1-Scores, ensuring that an entity is only marked \"correct\" if its type, start, and end are all perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b674837",
   "metadata": {},
   "source": [
    "## Step 1: Metric Configuration (Seqeval)\n",
    "We define the function that the trainer will use at each epoch. This function maps our numeric tags back to their clinical names (DISO, CHEM, etc.) and uses seqeval to judge the model's performance on full entities rather than individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2abc6122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# Mapping tags to their clinical labels for seqeval\n",
    "label_list = [\"O\", \"DISO\", \"PROC\", \"ANAT\", \"LIVB\", \"CHEM\", \"PHYS\", \"DEVI\", \"GEOG\", \"PHEN\", \"OBJC\"]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index -100 and map to string labels\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d479d",
   "metadata": {},
   "source": [
    "## Step 2: 5-Fold Cross-Validation Loop\n",
    "This is the core of Phase 2. We use KFold from Scikit-Learn to create 5 distinct training \"folds.\"\n",
    "Note: This process will train the model 5 times. If you are on a CPU, I recommend reducing n_splits to 2 for testing, but keep it at 5 for your final GitHub push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Cross-Validation with DrBERT...\n",
      "\n",
      "--- üè• Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2041/2041 [00:00<00:00, 6271.06 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 511/511 [00:00<00:00, 13813.95 examples/s]\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Dr-BERT/DrBERT-7GB and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\arshb\\AppData\\Local\\Temp\\ipykernel_21316\\904074250.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `CamembertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 34:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.935040</td>\n",
       "      <td>0.380329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.864846</td>\n",
       "      <td>0.446229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.871390</td>\n",
       "      <td>0.467171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PROC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: OBJC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PHYS seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: CHEM seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LIVB seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: DISO seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: ANAT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: GEOG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: DEVI seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PHEN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Strict F1: 0.4672\n",
      "\n",
      "--- üè• Fold 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2041/2041 [00:00<00:00, 5561.86 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 511/511 [00:00<00:00, 7091.14 examples/s]\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Dr-BERT/DrBERT-7GB and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 31:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.020945</td>\n",
       "      <td>0.367676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.855870</td>\n",
       "      <td>0.437758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.832995</td>\n",
       "      <td>0.451453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Strict F1: 0.4515\n",
      "\n",
      "--- üè• Fold 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2042/2042 [00:00<00:00, 6028.92 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 510/510 [00:00<00:00, 7732.05 examples/s]\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Dr-BERT/DrBERT-7GB and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 37:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.000529</td>\n",
       "      <td>0.365399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.932861</td>\n",
       "      <td>0.435588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.940697</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Strict F1: 0.4444\n",
      "\n",
      "--- üè• Fold 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2042/2042 [00:00<00:00, 4878.19 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 510/510 [00:00<00:00, 6491.10 examples/s]\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Dr-BERT/DrBERT-7GB and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 39:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.907523</td>\n",
       "      <td>0.341085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830673</td>\n",
       "      <td>0.429599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841363</td>\n",
       "      <td>0.444099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Strict F1: 0.4441\n",
      "\n",
      "--- üè• Fold 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2042/2042 [00:00<00:00, 4908.74 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 510/510 [00:00<00:00, 6281.39 examples/s]\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at Dr-BERT/DrBERT-7GB and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 37:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.977565</td>\n",
       "      <td>0.393737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.877662</td>\n",
       "      <td>0.450378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.879932</td>\n",
       "      <td>0.459078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Strict F1: 0.4591\n",
      "\n",
      "--- ‚úÖ Final K-Fold Results ---\n",
      "Mean Strict F1: 0.4532 (+/- 0.0089)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#DEFINE THE WEIGHTED TRAINER\n",
    "# This overrides the default loss to use clinical class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Move class_weights (from Phase 1) to the same device as the model (GPU/CPU)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "#INITIALIZE K-FOLD\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "# Prepare data directly from df columns\n",
    "data_list = df[['words', 'ner_tags']].to_dict('records')\n",
    "\n",
    "print(f\"Starting 5-Fold Cross-Validation with DrBERT...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(data_list)):\n",
    "    print(f\"\\n--- üè• Fold {fold + 1}/5 ---\")\n",
    "    \n",
    "    # Create the datasets for this specific fold\n",
    "    train_data = Dataset.from_list([data_list[i] for i in train_idx])\n",
    "    val_data = Dataset.from_list([data_list[i] for i in val_idx])\n",
    "    \n",
    "    # Tokenize and align using Phase 1 function\n",
    "    train_tokenized = train_data.map(tokenize_and_align_labels, batched=True)\n",
    "    val_tokenized = val_data.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "    # Load a fresh copy of DrBERT for each fold\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=11)\n",
    "\n",
    "    # Training settings\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./fold_{fold}\",\n",
    "        eval_strategy=\"epoch\",  \n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        num_train_epochs=3, \n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"no\", \n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer)\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_score = trainer.evaluate()['eval_f1']\n",
    "    fold_results.append(eval_score)\n",
    "    print(f\"Fold {fold+1} Strict F1: {eval_score:.4f}\")\n",
    "\n",
    "# Final Result\n",
    "print(\"\\n Final K-Fold Results\")\n",
    "print(f\"Mean Strict F1: {np.mean(fold_results):.4f} (+/- {np.std(fold_results):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"üöÄ Starting Final Production Training on 100% of data...\")\n",
    "\n",
    "# 1. Use ALL data (No train/val split this time)\n",
    "final_data_list = df[['words', 'ner_tags']].to_dict('records')\n",
    "final_dataset = Dataset.from_list(final_data_list)\n",
    "\n",
    "# 2. Apply your trusted alignment function\n",
    "final_tokenized = final_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# 3. Load a fresh, untrained DrBERT\n",
    "final_model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=11)\n",
    "\n",
    "# 4. Final Training Arguments (Saving the model this time!)\n",
    "final_args = TrainingArguments(\n",
    "    output_dir=\"./drbert-clinical-ner-final\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=4, # Slightly longer for the final run\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\", # Save at the end of every epoch\n",
    "    save_total_limit=1,    # Keep only the last (best) epoch to save disk space\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 5. Initialize the Trainer (Still using your Custom Weighted Loss)\n",
    "final_trainer = WeightedTrainer(\n",
    "    model=final_model,\n",
    "    args=final_args,\n",
    "    train_dataset=final_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer)\n",
    ")\n",
    "\n",
    "# 6. Train it!\n",
    "final_trainer.train()\n",
    "\n",
    "# 7. Explicitly save the final model and tokenizer to a dedicated folder\n",
    "save_path = \"./my_final_french_ner_model\"\n",
    "final_trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Production Model successfully saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71f283d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading your Custom French Medical AI...\n",
      "\n",
      "Analyzing text: 'Un patient de 68 ans avec des ant√©c√©dents d'hypertension art√©rielle et de diab√®te de type 2 est admis aux urgences pour une dyspn√©e s√©v√®re et des douleurs thoraciques irradiant vers le bras gauche. L'√©lectrocardiogramme a r√©v√©l√© une fibrillation auriculaire, justifiant l'administration intraveineuse de 40 mg de Furos√©mide et l'implantation d'un pacemaker temporaire.'\n",
      "\n",
      "/n-Extracted Clinical Entities -\n",
      " Term: patient         | Type: LIVB (√ätre Vivant) | Confidence: 1.00\n",
      " Term: 68 ans          | Type: LIVB (√ätre Vivant) | Confidence: 0.76\n",
      " Term: ant√©c√©dents     | Type: DISO (Maladie)     | Confidence: 0.65\n",
      " Term: 'hypertension art√©rielle | Type: DISO (Maladie)     | Confidence: 0.78\n",
      " Term: diab√®te de type | Type: DISO (Maladie)     | Confidence: 0.83\n",
      " Term: urgences        | Type: DISO (Maladie)     | Confidence: 0.37\n",
      " Term: dyspn√©e s√©v√®re  | Type: DISO (Maladie)     | Confidence: 0.88\n",
      " Term: douleurs        | Type: DISO (Maladie)     | Confidence: 1.00\n",
      " Term: thoraciques     | Type: ANAT (Anatomie)    | Confidence: 0.77\n",
      " Term: irradi          | Type: DISO (Maladie)     | Confidence: 0.67\n",
      " Term: bras gauche     | Type: ANAT (Anatomie)    | Confidence: 0.98\n",
      " Term: '√©lectrocardiogramme | Type: PROC (Proc√©dure)   | Confidence: 0.84\n",
      " Term: fibrillation auriculaire | Type: DISO (Maladie)     | Confidence: 0.85\n",
      " Term: administration intraveineuse | Type: PROC (Proc√©dure)   | Confidence: 0.94\n",
      " Term: Furos√©mide      | Type: CHEM (M√©dicament)  | Confidence: 1.00\n",
      " Term: implantation    | Type: PROC (Proc√©dure)   | Confidence: 0.84\n",
      " Term: pacemaker temporaire | Type: PHEN (Ph√©nom√®ne)   | Confidence: 0.89\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Point to the folder where we saved your model\n",
    "saved_model_path = \"./final_french_ner_model\"\n",
    "\n",
    "# Load your newly trained AI\n",
    "print(\"Loading your Custom French Medical AI...\")\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "custom_model = AutoModelForTokenClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "# Create a Hugging Face Pipeline for NER\n",
    "# We set aggregation_strategy=\"simple\" to group sub-tokens back into full words!\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", \n",
    "    model=custom_model, \n",
    "    tokenizer=custom_tokenizer, \n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "#The Test Sentence (A typical French clinical note)\n",
    "test_sentence = \"Un patient de 68 ans avec des ant√©c√©dents d'hypertension art√©rielle et de diab√®te de type 2 est admis aux urgences pour une dyspn√©e s√©v√®re et des douleurs thoraciques irradiant vers le bras gauche. L'√©lectrocardiogramme a r√©v√©l√© une fibrillation auriculaire, justifiant l'administration intraveineuse de 40 mg de Furos√©mide et l'implantation d'un pacemaker temporaire.\"\n",
    "\n",
    "print(f\"\\nAnalyzing text: '{test_sentence}'\\n\")\n",
    "\n",
    "#Run the AI!\n",
    "predictions = ner_pipeline(test_sentence)\n",
    "\n",
    "id2label = {\n",
    "    \"LABEL_0\": \"O\", \n",
    "    \"LABEL_1\": \"DISO (Maladie)\", \n",
    "    \"LABEL_2\": \"PROC (Proc√©dure)\", \n",
    "    \"LABEL_3\": \"ANAT (Anatomie)\", \n",
    "    \"LABEL_4\": \"LIVB (√ätre Vivant)\", \n",
    "    \"LABEL_5\": \"CHEM (M√©dicament)\", \n",
    "    \"LABEL_6\": \"PHYS (Physiologie)\", \n",
    "    \"LABEL_7\": \"DEVI (Appareil)\", \n",
    "    \"LABEL_8\": \"GEOG (Lieu)\", \n",
    "    \"LABEL_9\": \"PHEN (Ph√©nom√®ne)\", \n",
    "    \"LABEL_10\": \"OBJC (Objet)\"\n",
    "}\n",
    "\n",
    "# 7. Print the results nicely\n",
    "print(\"/n-Extracted Clinical Entities -\")\n",
    "if not predictions:\n",
    "    print(\"No entities found. (Try a longer medical sentence!)\")\n",
    "else:\n",
    "    for entity in predictions:\n",
    "        word = entity['word']\n",
    "        raw_label = entity['entity_group']\n",
    "        score = entity['score']\n",
    "        \n",
    "        # Format the output\n",
    "        readable_label = id2label.get(raw_label, raw_label)\n",
    "        \n",
    "        # We ignore 'O' (Outside) tags to only show the medical terms\n",
    "        if readable_label != \"O\":\n",
    "            print(f\" Term: {word:<15} | Type: {readable_label:<18} | Confidence: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45bbdb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Building the Semantic CIM-10 Ranker...\n",
      "\n",
      "-- Semantic Interoperability Results (T2A Mapping) --\n",
      "üî∏Terme Extrait : hypertension art√©rielle  \n",
      "   ‚Ü≥ Code CIM-10 : I10 (Hypertension art√©rielle essentielle) | Confiance: 0.86\n",
      "\n",
      "üî∏Terme Extrait : diab√®te de type 2        \n",
      "   ‚Ü≥ Code CIM-10 : E11.9 (Diab√®te sucr√© de type 2 sans complication) | Confiance: 0.66\n",
      "\n",
      "üî∏Terme Extrait : dyspn√©e s√©v√®re           \n",
      "   ‚Ü≥ Code CIM-10 : R06.0 (Dyspn√©e) | Confiance: 0.94\n",
      "\n",
      "üî∏Terme Extrait : fibrillation auriculaire \n",
      "   ‚Ü≥ Code CIM-10 : I48.9 (Fibrillation auriculaire non sp√©cifi√©e) | Confiance: 0.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"‚öôÔ∏è Building the Semantic CIM-10 Ranker...\")\n",
    "\n",
    "# Create a \"Mini\" CIM-10 Database (Simulating a real hospital database)/hardcoding\n",
    "cim10_data = {\n",
    "    \"Code\": [\"I10\", \"E11.9\", \"R06.0\", \"R07.4\", \"I48.9\", \"R51\", \"G03.9\"],\n",
    "    \"Description\": [\n",
    "        \"Hypertension art√©rielle essentielle\",\n",
    "        \"Diab√®te sucr√© de type 2 sans complication\",\n",
    "        \"Dyspn√©e\",\n",
    "        \"Douleur thoracique, sans pr√©cision\",\n",
    "        \"Fibrillation auriculaire non sp√©cifi√©e\",\n",
    "        \"C√©phal√©e\",\n",
    "        \"M√©ningite, non sp√©cifi√©e\"\n",
    "    ]\n",
    "}\n",
    "df_cim10 = pd.DataFrame(cim10_data)\n",
    "\n",
    "# Build the Mathematical Search Engine (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4))\n",
    "tfidf_matrix = vectorizer.fit_transform(df_cim10['Description'])\n",
    "\n",
    "# Define the Mapping Function\n",
    "def get_cim10_code(extracted_term):\n",
    "    # Convert the extracted word into numbers\n",
    "    term_vector = vectorizer.transform([extracted_term])\n",
    "    \n",
    "    # Calculate Cosine Similarity against all codes in the database\n",
    "    similarities = cosine_similarity(term_vector, tfidf_matrix)\n",
    "    \n",
    "    # Get the best match\n",
    "    best_match_idx = similarities.argmax()\n",
    "    best_score = similarities[0, best_match_idx]\n",
    "    \n",
    "    # If the similarity score is decent (> 0.25), return the code\n",
    "    if best_score > 0.25:\n",
    "        return df_cim10.iloc[best_match_idx]['Code'], df_cim10.iloc[best_match_idx]['Description'], best_score\n",
    "    else:\n",
    "        return \"N/A\", \"Aucun code correspondant\", best_score\n",
    "\n",
    "# Letz test it using the exact output from your Phase 2 test!\n",
    "test_extracted_diseases = [\n",
    "    \"hypertension art√©rielle\", \n",
    "    \"diab√®te de type 2\", \n",
    "    \"dyspn√©e s√©v√®re\", \n",
    "    \"fibrillation auriculaire\"\n",
    "]\n",
    "\n",
    "print(\"\\n-- Semantic Interoperability Results (T2A Mapping) --\")\n",
    "for disease in test_extracted_diseases:\n",
    "    code, desc, score = get_cim10_code(disease)\n",
    "    print(f\"üî∏Terme Extrait : {disease:<25}\")\n",
    "    print(f\"   ‚Ü≥ Code CIM-10 : {code} ({desc}) | Confiance: {score:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4e470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RGPD Pseudonymization Pipeline...\n",
      "\n",
      "--- üõë AVANT RGPD (Texte original, non-s√©curis√©) ---\n",
      "Un patient de 68 ans avec des ant√©c√©dents d'hypertension art√©rielle est admis aux urgences de l'H√¥pital Piti√©-Salp√™tri√®re pour une dyspn√©e.\n",
      "\n",
      "--- ‚úÖ APR√àS RGPD (Pseudonymis√©, pr√™t pour la base de donn√©es) ---\n",
      "Un[DONN√âE_PATIENT] avec des ant√©c√©dents d'hypertension art√©rielle est admis aux urgences de l'H√¥pital[LIEU_G√âOGRAPHIQUE]-[LIEU_G√âOGRAPHIQUE] pour une dyspn√©e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arshb\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `CamembertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing RGPD Pseudonymization Pipeline...\\n\")\n",
    "\n",
    "# Define our new test sentence with a Geography (GEOG) added\n",
    "test_sentence_rgpd = \"Un patient de 68 ans avec des ant√©c√©dents d'hypertension art√©rielle est admis aux urgences de l'H√¥pital Piti√©-Salp√™tri√®re pour une dyspn√©e.\"\n",
    "\n",
    "predictions = ner_pipeline(test_sentence_rgpd)\n",
    "\n",
    "#  The Scrubber Function\n",
    "def pseudonymize_clinical_note(text, ner_results):\n",
    "    scrubbed_text = text\n",
    "    \n",
    "    # We target LABEL_4 (LIVB / Patients) and LABEL_8 (GEOG / Locations)\n",
    "    sensitive_tags = {\n",
    "        \"LABEL_4\": \"[DONN√âE_PATIENT]\", \n",
    "        \"LABEL_8\": \"[LIEU_G√âOGRAPHIQUE]\"\n",
    "    }\n",
    "\n",
    "    # We sort the entities in REVERSE order based on where they start.\n",
    "    sorted_entities = sorted(ner_results, key=lambda x: x['start'], reverse=True)\n",
    "\n",
    "    for ent in sorted_entities:\n",
    "        raw_label = ent['entity_group']\n",
    "        \n",
    "        # If the AI flagged it as sensitive, we scrub it\n",
    "        if raw_label in sensitive_tags:\n",
    "            start = ent['start']\n",
    "            end = ent['end']\n",
    "            placeholder = sensitive_tags[raw_label]\n",
    "            \n",
    "            #slice the string to inject the placeholder\n",
    "            scrubbed_text = scrubbed_text[:start] + placeholder + scrubbed_text[end:]\n",
    "\n",
    "    return scrubbed_text\n",
    "\n",
    "# execute the Scrubber\n",
    "safe_text = pseudonymize_clinical_note(test_sentence_rgpd, predictions)\n",
    "\n",
    "#Display the compliance results\n",
    "print(\"--- üõë AVANT RGPD (Texte original, non-s√©curis√©) ---\")\n",
    "print(test_sentence_rgpd)\n",
    "print(\"\\n--- ‚úÖ APR√àS RGPD (Pseudonymis√©, pr√™t pour la base de donn√©es) ---\")\n",
    "print(safe_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
